{"cells":[{"cell_type":"code","execution_count":41,"id":"04232fd2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"04232fd2","executionInfo":{"status":"ok","timestamp":1746497327432,"user_tz":-480,"elapsed":931289,"user":{"displayName":"Brett Lim","userId":"01676062279260287477"}},"outputId":"25f31949-d2b1-4cbb-cca7-ef2fcece5499"},"outputs":[{"output_type":"stream","name":"stdout","text":["W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","(Reading database ... 126257 files and directories currently installed.)\n","Preparing to unpack /tmp/chrome.deb ...\n","Unpacking google-chrome-stable (136.0.7103.59-1) over (136.0.7103.59-1) ...\n","Setting up google-chrome-stable (136.0.7103.59-1) ...\n","Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Google Chrome 136.0.7103.59 \n","Loading videoâ€¦\n"]},{"output_type":"stream","name":"stderr","text":["Crawling:   3%|â–         | 20/600 [14:56<7:13:28, 44.84s/it]"]},{"output_type":"stream","name":"stdout","text":["No comment growth in 20 cycles â€“ stopping.\n","Captured 330 comments\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["âœ… Outputs in outputs\n"]}],"source":["# FacebookÂ VideoÂ ScraperÂ (Colab)Â â€“Â Stable Comment Crawler (fixed syntax)\n","# =====================================================================\n","# â€¢ Robust button selectors: span â†’ ancestor div[@role=button]\n","# â€¢ Avoids stale element refs; scrolls with JS.\n","# â€¢ Stops when no pagination buttons remain **and** pager shows `Nâ€¯ofâ€¯N`,\n","#   or when comment count stagnates for 15 cycles.\n","# â€¢ Exports CSV, TXT, screenshot, and OCR.\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CellÂ 1Â : Environment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","!apt-get update -qq\n","!apt-get install -y wget unzip curl gnupg2 tesseract-ocr -qq\n","!wget -q -O /tmp/chrome.deb https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n","!dpkg -i /tmp/chrome.deb || apt-get -fy install -qq\n","!google-chrome --version\n","!pip install selenium==4.10.0 chromedriver-autoinstaller pillow pytesseract pandas tqdm -q\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CellÂ 2Â : Scraper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","import os, time, re, pytesseract, pandas as pd\n","from PIL import Image\n","from tqdm import tqdm\n","import chromedriver_autoinstaller as cda\n","from selenium import webdriver\n","from selenium.webdriver.chrome.options import Options\n","from selenium.webdriver.chrome.service import Service\n","from selenium.webdriver.common.by import By\n","from selenium.webdriver.support.ui import WebDriverWait\n","from selenium.webdriver.support import expected_conditions as EC\n","from selenium.common.exceptions import (\n","    TimeoutException, NoSuchElementException, StaleElementReferenceException,\n","    ElementClickInterceptedException)\n","\n","VIDEO_URL  = \"https://www.facebook.com/AlodiaGosiengfiao/videos/648653461336102/\"\n","OUTPUT_DIR = \"outputs\"; os.makedirs(OUTPUT_DIR, exist_ok=True)\n","WAIT = 0.8\n","\n","# â”€â”€ Driver â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","path = cda.install()\n","opts = Options(); opts.add_argument(\"--headless=new\"); opts.add_argument(\"--no-sandbox\"); opts.add_argument(\"--disable-dev-shm-usage\")\n","service = Service(path)\n","driver = webdriver.Chrome(service=service, options=opts)\n","wait   = WebDriverWait(driver, 25)\n","\n","# â”€â”€ Load page & close modal â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","print(\"Loading videoâ€¦\")\n","driver.get(VIDEO_URL)\n","wait.until(lambda d: d.execute_script(\"return document.readyState\") == \"complete\")\n","try:\n","    dlg = wait.until(EC.presence_of_element_located((By.XPATH, '//div[@role=\"dialog\"]')))\n","    dlg.find_element(By.XPATH, './/div[@role=\"button\" and contains(@aria-label,\"Close\")]').click()\n","    time.sleep(1)\n","except TimeoutException:\n","    pass\n","\n","# â”€â”€ Selectors â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","PAGE_KEYWORDS = {\n","    \"View more comments\":    \"//span[normalize-space(text())='View more comments']/ancestor::div[@role='button']\",\n","    \"View previous comments\": \"//span[normalize-space(text())='View previous comments']/ancestor::div[@role='button']\",\n","    \"View more replies\":     \"//span[normalize-space(text())='View more replies']/ancestor::div[@role='button']\",\n","    \"See more replies\":      \"//span[normalize-space(text())='See more replies']/ancestor::div[@role='button']\"\n","}\n","LONG_BTN_XP  = \"//span[normalize-space(text())='See more']/ancestor::div[@role='button']\"\n","ARTICLES_XP  = \"//div[@role='article' and starts-with(@aria-label,'Comment by')]\"\n","PAGER_XP     = \"//span[contains(text(),' of ')]\"\n","\n","# â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","\n","def safe_click(el):\n","    try:\n","        driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", el)\n","        time.sleep(0.1)\n","        el.click(); return True\n","    except (StaleElementReferenceException, ElementClickInterceptedException):\n","        return False\n","\n","\n","def click_pagination_buttons():\n","    \"\"\"Click all pagination buttons across all keyword XPaths until none remain.\"\"\"\n","    clicked = False\n","    for xp in PAGE_KEYWORDS.values():\n","        while True:\n","            btns = driver.find_elements(By.XPATH, xp)\n","            if not btns: break\n","            for b in btns:\n","                if safe_click(b): clicked = True\n","            time.sleep(0.2)\n","    return clicked\n","\n","\n","def click_long_comment_buttons():\n","    for b in driver.find_elements(By.XPATH, LONG_BTN_XP):\n","        safe_click(b)\n","\n","\n","def scrape_comments():\n","    rows = []\n","    for art in driver.find_elements(By.XPATH, ARTICLES_XP):\n","        author = \"\"; ts = \"\"; comment = \"\"\n","        try:\n","            author = art.find_element(By.XPATH, \".//a[1]//span[@dir='auto']\").text\n","        except NoSuchElementException: pass\n","        try:\n","            ts = art.find_element(By.XPATH, \".//ul//a[1]\").text\n","        except NoSuchElementException: pass\n","        parts = [d.text for d in art.find_elements(By.XPATH, \".//div[@dir='auto']\") if d.text.strip()]\n","        if parts: comment = max(parts, key=len)\n","        if comment:\n","            rows.append({\"author\": author, \"timestamp\": ts, \"comment\": comment})\n","    return rows\n","\n","\n","def pager_done():\n","    try:\n","        txt = driver.find_element(By.XPATH, PAGER_XP).text.strip()\n","        a, b = re.match(r\"(\\d+)\\s+of\\s+(\\d+)\", txt).groups(); return a == b\n","    except Exception: return False\n","\n","# â”€â”€ Crawl â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","all_rows, stagn, prev_len = [], 0, 0\n","for _ in tqdm(range(600), desc=\"Crawling\"):\n","    paged = click_pagination_buttons()\n","    click_long_comment_buttons()\n","    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n","    time.sleep(WAIT)\n","    all_rows = scrape_comments()\n","    if len(all_rows) == prev_len: stagn += 1\n","    else: stagn = 0\n","    prev_len = len(all_rows)\n","    if not paged and pager_done():\n","        print(\"Pager indicates last slice â€“ stopping.\"); break\n","    if stagn >= 20:\n","        print(\"No comment growth in 20 cycles â€“ stopping.\"); break\n","\n","print(\"Captured\", len(all_rows), \"comments\")\n","\n","# â”€â”€ Save â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","import pandas as pd\n","pd.DataFrame(all_rows).to_csv(os.path.join(OUTPUT_DIR, \"facebook_comments.csv\"), index=False)\n","with open(os.path.join(OUTPUT_DIR, \"facebook_comments.txt\"), \"w\", encoding=\"utf-8\") as f:\n","    f.write(\"\\n\".join(r['comment'] for r in all_rows))\n","\n","shot = os.path.join(OUTPUT_DIR, \"screenshot.png\"); ocr = os.path.join(OUTPUT_DIR, \"ocr.txt\")\n","driver.save_screenshot(shot)\n","with open(ocr, \"w\", encoding=\"utf-8\") as f:\n","    f.write(pytesseract.image_to_string(Image.open(shot)))\n","\n","print(\"âœ… Outputs in\", OUTPUT_DIR)\n","driver.quit()\n"]},{"cell_type":"code","source":["# FacebookÂ VideoÂ ScraperÂ (Colab)Â â€“Â Allâ€‘Comments Mode\n","# ====================================================\n","# NewÂ features\n","# ------------\n","# â˜… Switches comment filter from **â€œMostâ€¯relevantâ€** (default) â†’ **â€œAllâ€¯commentsâ€.**\n","# â˜… Then exhaustively clicks every pagination control to reach allâ€¯462â€¯comments.\n","# â˜… Outputs CSV, TXT, screenshot, OCR, plus console preview.\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CellÂ 1Â : Environment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","!apt-get update -qq\n","!apt-get install -y wget unzip curl gnupg2 tesseract-ocr -qq\n","!wget -q -O /tmp/chrome.deb https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n","!dpkg -i /tmp/chrome.deb || apt-get -fy install -qq\n","!google-chrome --version\n","!pip install selenium==4.10.0 chromedriver-autoinstaller pillow pytesseract pandas tqdm -q\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CellÂ 2Â : Scraper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","import os, time, re, pytesseract, pandas as pd\n","from PIL import Image\n","from tqdm import tqdm\n","import chromedriver_autoinstaller as cda\n","from selenium import webdriver\n","from selenium.webdriver.chrome.options import Options\n","from selenium.webdriver.chrome.service import Service\n","from selenium.webdriver.common.by import By\n","from selenium.webdriver.support.ui import WebDriverWait\n","from selenium.webdriver.support import expected_conditions as EC\n","from selenium.common.exceptions import (\n","    TimeoutException, NoSuchElementException, StaleElementReferenceException,\n","    ElementClickInterceptedException)\n","\n","VIDEO_URL  = \"https://www.facebook.com/AlodiaGosiengfiao/videos/648653461336102/\"\n","OUTPUT_DIR = \"outputs\"; os.makedirs(OUTPUT_DIR, exist_ok=True)\n","WAIT = 0.8\n","\n","# â”€â”€ Driver â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","path = cda.install()\n","opts = Options()\n","opts.add_argument(\"--headless=new\"); opts.add_argument(\"--no-sandbox\"); opts.add_argument(\"--disable-dev-shm-usage\")\n","service = Service(path)\n","driver = webdriver.Chrome(service=service, options=opts)\n","wait   = WebDriverWait(driver, 25)\n","\n","# â”€â”€ Load page & close modal â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","print(\"Loading videoâ€¦\")\n","driver.get(VIDEO_URL)\n","wait.until(lambda d: d.execute_script(\"return document.readyState\") == \"complete\")\n","try:\n","    dlg = wait.until(EC.presence_of_element_located((By.XPATH, '//div[@role=\"dialog\"]')))\n","    dlg.find_element(By.XPATH, './/div[@role=\"button\" and contains(@aria-label,\"Close\")]').click(); time.sleep(1)\n","except TimeoutException:\n","    pass\n","\n","# â”€â”€ Switch filter to â€œAll commentsâ€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","try:\n","    filt_btn = wait.until(EC.element_to_be_clickable((By.XPATH,\n","        \"//span[normalize-space(text())='Most relevant' or normalize-space(text())='All comments' or normalize-space(text())='Newest']/ancestor::div[@role='button']\")))\n","    driver.execute_script(\"arguments[0].click();\", filt_btn); time.sleep(0.5)\n","    all_opt = wait.until(EC.element_to_be_clickable((By.XPATH,\n","        \"//span[normalize-space(text())='All comments']/ancestor::div[@role='menuitem']\")))\n","    driver.execute_script(\"arguments[0].click();\", all_opt); time.sleep(1)\n","    print(\"ğŸ”„ Filter set to All comments\")\n","except TimeoutException:\n","    print(\"âš ï¸  Could not switch filter â€“ proceeding with current setting\")\n","\n","# â”€â”€ Selectors â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","PAGE_KEYWORDS = {\n","    \"View more comments\":    \"//span[normalize-space(text())='View more comments']/ancestor::div[@role='button']\",\n","    \"View previous comments\": \"//span[normalize-space(text())='View previous comments']/ancestor::div[@role='button']\",\n","    \"View more replies\":     \"//span[normalize-space(text())='View more replies']/ancestor::div[@role='button']\",\n","    \"See more replies\":      \"//span[normalize-space(text())='See more replies']/ancestor::div[@role='button']\"\n","}\n","LONG_BTN_XP = \"//span[normalize-space(text())='See more']/ancestor::div[@role='button']\"\n","ARTICLES_XP = \"//div[@role='article' and starts-with(@aria-label,'Comment by')]\"\n","PAGER_XP    = \"//span[contains(text(),' of ')]\"\n","\n","# â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","\n","def safe_click(el):\n","    try:\n","        driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", el); time.sleep(0.1)\n","        el.click(); return True\n","    except (StaleElementReferenceException, ElementClickInterceptedException):\n","        return False\n","\n","\n","def click_pagination_buttons():\n","    clicked = False\n","    for xp in PAGE_KEYWORDS.values():\n","        while True:\n","            btns = driver.find_elements(By.XPATH, xp)\n","            if not btns: break\n","            for b in btns:\n","                if safe_click(b): clicked = True\n","            time.sleep(0.2)\n","    return clicked\n","\n","\n","def click_long_comment_buttons():\n","    for b in driver.find_elements(By.XPATH, LONG_BTN_XP): safe_click(b)\n","\n","\n","def scrape_comments():\n","    recs = []\n","    for art in driver.find_elements(By.XPATH, ARTICLES_XP):\n","        try: author = art.find_element(By.XPATH, \".//a[1]//span[@dir='auto']\").text\n","        except NoSuchElementException: author = \"\"\n","        try: ts = art.find_element(By.XPATH, \".//ul//a[1]\").text\n","        except NoSuchElementException: ts = \"\"\n","        txts = [d.text for d in art.find_elements(By.XPATH, \".//div[@dir='auto']\") if d.text.strip()]\n","        if txts:\n","            recs.append({\"author\": author, \"timestamp\": ts, \"comment\": max(txts, key=len)})\n","    return recs\n","\n","\n","def pager_done():\n","    try:\n","        a,b = re.match(r\"(\\d+)\\s+of\\s+(\\d+)\", driver.find_element(By.XPATH, PAGER_XP).text).groups(); return a==b\n","    except Exception: return False\n","\n","# â”€â”€ Crawl â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","all_rows, stagn, prev = [], 0, 0\n","for _ in tqdm(range(800), desc=\"Expanding\"):\n","    paged = click_pagination_buttons(); click_long_comment_buttons()\n","    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\"); time.sleep(WAIT)\n","    all_rows = scrape_comments()\n","    if len(all_rows)==prev: stagn+=1\n","    else: stagn=0\n","    prev = len(all_rows)\n","    if pager_done() and not paged:\n","        print(\"âœ”ï¸ Pager shows last slice & no buttons left\"); break\n","    if stagn>=25:\n","        print(\"âš ï¸  No growth for 25 loops â€“ aborting\"); break\n","\n","print(\"Total comments captured:\", len(all_rows))\n","\n","# â”€â”€ Save & preview â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","csv_path = os.path.join(OUTPUT_DIR, \"facebook_comments.csv\")\n","pd.DataFrame(all_rows).to_csv(csv_path, index=False, encoding=\"utf-8\")\n","with open(os.path.join(OUTPUT_DIR, \"facebook_comments.txt\"), \"w\", encoding=\"utf-8\") as f:\n","    f.write(\"\\n\".join(r['comment'] for r in all_rows))\n","print(\"CSV â†’\", csv_path, \"| size\", os.path.getsize(csv_path), \"bytes\")\n","try:\n","    print(pd.read_csv(csv_path, nrows=3))\n","except Exception as e:\n","    print(\"Preview failed\", e)\n","\n","shot = os.path.join(OUTPUT_DIR, \"screenshot.png\"); ocr = os.path.join(OUTPUT_DIR, \"ocr.txt\")\n","driver.save_screenshot(shot)\n","with open(ocr, \"w\", encoding=\"utf-8\") as f: f.write(pytesseract.image_to_string(Image.open(shot)))\n","print(\"âœ… Outputs in\", OUTPUT_DIR)\n","driver.quit()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YTQpLeppbqIX","executionInfo":{"status":"ok","timestamp":1746500190062,"user_tz":-480,"elapsed":1463110,"user":{"displayName":"Brett Lim","userId":"01676062279260287477"}},"outputId":"1ba6b5bd-f803-4b53-8f63-f04aa9f75882"},"id":"YTQpLeppbqIX","execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","(Reading database ... 126257 files and directories currently installed.)\n","Preparing to unpack /tmp/chrome.deb ...\n","Unpacking google-chrome-stable (136.0.7103.59-1) over (136.0.7103.59-1) ...\n","Setting up google-chrome-stable (136.0.7103.59-1) ...\n","Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Google Chrome 136.0.7103.59 \n","Loading videoâ€¦\n","ğŸ”„ Filter set to All comments\n"]},{"output_type":"stream","name":"stderr","text":["Expanding:   3%|â–         | 25/800 [23:45<12:16:18, 57.00s/it]"]},{"output_type":"stream","name":"stdout","text":["âš ï¸  No growth for 25 loops â€“ aborting\n","Total comments captured: 422\n","CSV â†’ outputs/facebook_comments.csv | size 17327 bytes\n","               author timestamp  \\\n","0  Alodia Gosiengfiao        1w   \n","1       The Butch Tan        1w   \n","2    Franco Pantangco        1w   \n","\n","                                             comment  \n","0  Shrimptastic Magic Balls: https://www.nestlego...  \n","1  Great idea to ah! Push lang, papatok tong gant...  \n","2       Are you playing Clair Obscur: Expedition 33?  \n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["âœ… Outputs in outputs\n"]}]}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}